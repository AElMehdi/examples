# Confluent Cloud

# Confluent Cloud configuration file
# - File should include the following configuration parameters
# bootstrap.servers=<BROKER ENDPOINT>
# ssl.endpoint.identification.algorithm=https
# security.protocol=SASL_SSL
# sasl.mechanism=PLAIN
# sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username\="<API KEY>" password\="<API SECRET>";
# schema.registry.url=https://<SR ENDPOINT>
# schema.registry.basic.auth.user.info=<SR API KEY>:<SR API SECRET>
# basic.auth.credentials.source=USER_INFO
# ksql.endpoint=https://<KSQL ENDPOINT>
# ksql.basic.auth.user.info=<KSQL API KEY>:<KSQL KSQL SECRET>
CONFIG_FILE=~/.ccloud/config



# Kinesis

# KINESIS_STREAM_NAME: will be created and deleted by the demo
export KINESIS_STREAM_NAME='demo-logs'

export KINESIS_REGION='us-west-2'

# AWS_PROFILE: profile must exist in ~/.aws/credentials
export AWS_PROFILE=default



# Cloud storage sink
# These parameters need to be changed depending on which storage cloud provider you are using

# DESTINATION_STORAGE can be one of 's3' or 'gcs' or 'az'
export DESTINATION_STORAGE='az'

export STORAGE_REGION='us-west-2'

# STORAGE_PROFILE: profile (for AWS S3) or account (for Azure Blob)
# - AWS S3: profile must exist in ~/.aws/credentials
# - AZ Blob: provide name of storage account must exist, with keys
export STORAGE_PROFILE=default

# STORAGE_BUCKET_NAME: bucket (for AWS S3) or container (for Azure Blob)
# - AWS S3: bucket name
# - AZ Blob: container name
# Demo will modify contents of the bucket/container 
# Do not specify one that you do not want accidentally deleted
export STORAGE_BUCKET_NAME='confluent-cloud-demo'



# Do not modify topic names
# These must stay in sync with the ksql.commands and ksql.cleanup.commands files
export KAFKA_TOPIC_NAME_IN='eventLogs'
export KAFKA_TOPIC_NAME_OUT1='SUM_PER_SOURCE'
export KAFKA_TOPIC_NAME_OUT2='COUNT_PER_SOURCE'
